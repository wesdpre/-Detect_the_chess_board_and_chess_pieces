{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaU9_sOGzRD1"
   },
   "source": [
    "# Task 2 Multiclass classification Baseline\n",
    "\n",
    "In this class, we will develop a baseline for Task 2 using the chess dataset. We will model the task as an multiclass classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dvfTDUXuzRD9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np, os, torch, random, cv2, json\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.ops as ops\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCiAkFspd8US"
   },
   "source": [
    "### Connect Colab to Drive (if the dataset is on drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDBhCwhszREA"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "**Tip**: since the images are very big, resize the dataset before loading it to save time and memory during training (use cubic interpolation to preserve image quality when downsizing the images)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w5IQVCmOzREB"
   },
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "data_aug = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_in = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juTsr3Lld8UT",
    "outputId": "4f307a21-f67a-4229-8c51-ea90ec364a25"
   },
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, root_dir, partition, transform=None):\n",
    "        self.root = root_dir\n",
    "        self.anns = json.load(open(os.path.join(root_dir, 'annotations.json')))\n",
    "\n",
    "        self.id_to_category = {c['id']: c['name'] for c in self.anns['categories']}\n",
    "        self.category_to_id = {c['name']: c['id'] for c in self.anns['categories']}\n",
    "        self.categories = [c['name'] for c in self.anns['categories']]\n",
    "\n",
    "        # --- Step 1: Pre-process ALL annotations and identify problematic images ---\n",
    "        self.all_image_ids_from_json = []\n",
    "        self.all_file_names_from_json = []\n",
    "        for img_info in self.anns['images']:\n",
    "            self.all_file_names_from_json.append(img_info['path'])\n",
    "            self.all_image_ids_from_json.append(img_info['id'])\n",
    "        self.all_image_ids_from_json = np.asarray(self.all_image_ids_from_json) # Convert to numpy for easy indexing/masking\n",
    "\n",
    "        problematic_image_ids = set() # Store image_ids that have any malformed piece annotation\n",
    "        # This will store ONLY VALID annotations, grouped by image_id\n",
    "        self.image_annotations = {}\n",
    "\n",
    "        for ann_idx, ann in enumerate(self.anns['annotations']['pieces']):\n",
    "            # Ensure required attributes for object detection are present\n",
    "            if 'bbox' not in ann or 'category_id' not in ann:\n",
    "                #print(f\"Warning: Annotation at index {ann_idx} for image_id {ann.get('image_id', 'N/A')} is malformed (missing 'bbox' or 'category_id'). This image will be excluded.\")\n",
    "                if 'image_id' in ann:\n",
    "                    problematic_image_ids.add(ann['image_id'])\n",
    "                continue # Skip this malformed annotation\n",
    "\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.image_annotations:\n",
    "                self.image_annotations[image_id] = []\n",
    "            self.image_annotations[image_id].append(ann)\n",
    "\n",
    "\n",
    "        # --- Step 2: Filter images based on split and problematic annotations ---\n",
    "        if partition == 'train':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['train']['image_ids']).astype(int)\n",
    "        elif partition == 'valid':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['val']['image_ids']).astype(int)\n",
    "        else: # 'test'\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['test']['image_ids']).astype(int)\n",
    "\n",
    "        self.file_names = [] # Final list of file names for this split\n",
    "        self.image_ids = []  # Final list of original image IDs for this split\n",
    "\n",
    "        # Iterate through all images (which you already have indexed by self.all_image_ids_from_json)\n",
    "        for i, img_id in enumerate(self.all_image_ids_from_json):\n",
    "            # Check if image belongs to current split AND is not marked as problematic\n",
    "            if img_id in raw_split_image_ids and img_id not in problematic_image_ids:\n",
    "                # Also, ensure that the image actually has valid annotations after filtering.\n",
    "                # An image could be in the split, not problematic, but simply have no pieces.\n",
    "                # Or all its pieces were problematic, so it has no valid annotations left.\n",
    "                if img_id in self.image_annotations and len(self.image_annotations[img_id]) > 0:\n",
    "                    self.image_ids.append(img_id)\n",
    "                    self.file_names.append(self.all_file_names_from_json[i])\n",
    "                else:\n",
    "                    # Optional: Print why an image might be excluded if it's in the split but has no valid annotations\n",
    "                    print(f\"Info: Image ID {img_id} (file: {self.all_file_names_from_json[i]}) in '{partition}' split has no valid annotations after initial filtering. Excluding.\")\n",
    "\n",
    "        self.file_names = np.asarray(self.file_names)\n",
    "        self.image_ids = np.asarray(self.image_ids)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Number of {partition} images: {len(self.file_names)}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.file_names[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_image_id = self.image_ids[idx] # Get the true original ID for this image in the split\n",
    "        # Retrieve annotations using the pre-processed dictionary\n",
    "        annotations_for_image = self.image_annotations.get(original_image_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # Iterate through the (already filtered and valid) annotations for this image\n",
    "        for ann in annotations_for_image:\n",
    "            x_min, y_min, width, height = ann['bbox']\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) if len(labels) > 0 else torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        # Create the target dictionary required by Faster R-CNN\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([original_image_id])\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "train_dataset = ChessDataset('./chessred2k', 'train', data_aug)\n",
    "valid_dataset = ChessDataset('./chessred2k', 'valid', data_in)\n",
    "test_dataset = ChessDataset('./chessred2k', 'test', data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAYHXPnzd8UT",
    "outputId": "199a5237-a064-457b-da4f-a7184019595f"
   },
   "outputs": [],
   "source": [
    "# Device configuration (improved version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparameters (consider these adjustments)\n",
    "batch_size = 8  # Reduced from 16 for better memory management with Faster R-CNN\n",
    "num_workers = 0  # Optimal for most systems\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Enhanced collate function for object detection.\n",
    "    Handles:\n",
    "    - Empty targets\n",
    "    - Image tensor conversion\n",
    "    - Device movement\n",
    "    \"\"\"\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    processed_images = []\n",
    "    processed_targets = []\n",
    "    \n",
    "    for image, target in zip(images, targets):\n",
    "        # Convert image if not already a tensor\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Handle empty targets\n",
    "        if len(target['boxes']) == 0:\n",
    "            target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            # Verify bbox format (x1,y1,x2,y2)\n",
    "            assert target['boxes'].shape[1] == 4, \"Bounding boxes must have shape [N, 4]\"\n",
    "            \n",
    "        processed_images.append(image)\n",
    "        processed_targets.append(target)\n",
    "    \n",
    "    return processed_images, processed_targets\n",
    "\n",
    "# DataLoaders with improved settings\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Prevents partial batches\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Keep all validation samples\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Often better to use batch_size=1 for testing\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets) in enumerate(valid_dataloader):\n",
    "    print(f\"\\nBatch {i}:\")\n",
    "    for j, t in enumerate(targets):\n",
    "        print(f\"  Target {j}:\")\n",
    "        print(f\"    Boxes shape: {t['boxes'].shape}\")\n",
    "        print(f\"    Labels shape: {t['labels'].shape}\")\n",
    "        if t['boxes'].shape[0] != t['labels'].shape[0]:\n",
    "            print(\"    Inconsistency!\")\n",
    "    \n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the dataset is loaded correctly\n",
    "for i in range(len(valid_dataset)):\n",
    "    _, target = valid_dataset[i]\n",
    "    assert target['boxes'].dim() == 2 and target['boxes'].shape[1] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating dataset with pieces\n",
    "\n",
    "def plot_boxes(image, boxes, title=\"Detections\"):\n",
    "    plt.imshow(image)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso con tus datos:\n",
    "image, target = train_dataset[10]\n",
    "denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "             torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "denorm_img = np.clip(denorm_img, 0, 1)\n",
    "\n",
    "plot_boxes(denorm_img, target['boxes'], \"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aqWR_0VzRED"
   },
   "source": [
    "## Defining the model\n",
    "\n",
    "We will use a pre-trained ResNet50 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfK3c9RSzRED",
    "outputId": "9cd92465-f107-4511-80a2-34f07fd16a33"
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "\n",
    "num_classes = 12  # 12 pieces + empty space\n",
    "    \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "step_size = 3 \n",
    "gamma = 0.1\n",
    "\n",
    "# Optimizador y scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=learning_rate, \n",
    "                     momentum=momentum, \n",
    "                     weight_decay=weight_decay)\n",
    "lr_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0).item()\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0).item()\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0).item()\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0).item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': total_loss/(progress_bar.n+1),\n",
    "            'Cls': loss_classifier/(progress_bar.n+1),\n",
    "            'Box': loss_box_reg/(progress_bar.n+1)\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0\n",
    "    \n",
    "    # Configuración especial para evitar errores de dimensiones\n",
    "    original_score_thresh = model.roi_heads.score_thresh\n",
    "    model.roi_heads.score_thresh = 0.0  # Desactiva filtrado inicial\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc='Validating')):\n",
    "        # 1. Conversión explícita y verificación\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{\n",
    "            'boxes': t['boxes'].float().to(device),\n",
    "            'labels': t['labels'].long().to(device)\n",
    "        } for t in targets]\n",
    "        \n",
    "        # 3. Forward pass con manejo de excepciones interno\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # 4. Cálculo de pérdida unificado\n",
    "        if isinstance(loss_dict, dict):\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        else:  # Caso raro de lista de diccionarios\n",
    "            losses = sum(sum(l.values()) for l in loss_dict)\n",
    "        \n",
    "        total_loss += losses\n",
    "        processed_batches += 1\n",
    "                \n",
    "    \n",
    "    # Restaura configuración original\n",
    "    model.roi_heads.score_thresh = original_score_thresh\n",
    "\n",
    "    return total_loss / processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "def train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs, early_stop_patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        train_loss = train_one_epoch(model, optimizer, train_dataloader, device, epoch)\n",
    "        \n",
    "        # Validación\n",
    "        val_loss = evaluate(model, valid_dataloader, device)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            patience_counter = 0\n",
    "            print(f\"¡Nuevo mejor modelo guardado! (val_loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping en epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Log de progreso\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(\"¡Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nms(predictions, iou_threshold=0.5):\n",
    "    filtered_preds = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        boxes = pred['boxes']\n",
    "        scores = pred['scores']\n",
    "        labels = pred['labels']\n",
    "\n",
    "        if boxes.numel() == 0:\n",
    "            filtered_preds.append(pred)\n",
    "            continue\n",
    "\n",
    "        # Apply NMS with no pre-filtering\n",
    "        keep_idxs = ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "        # Keep only those selected by NMS\n",
    "        filtered_preds.append({\n",
    "            'boxes': boxes[keep_idxs],\n",
    "            'scores': scores[keep_idxs],\n",
    "            'labels': labels[keep_idxs]\n",
    "        })\n",
    "\n",
    "    return filtered_preds\n",
    "\n",
    "def plot_predictions(model, dataset, num_samples=3):\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, target = dataset[idx]\n",
    "        img_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(img_tensor)\n",
    "        \n",
    "        # Procesamiento de la imagen para visualización\n",
    "        denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "                     torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "        denorm_img = np.clip(denorm_img, 0, 1)\n",
    "        \n",
    "        # Visualizar\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 7))\n",
    "        \n",
    "        # Ground Truth\n",
    "        ax[0].imshow(denorm_img)\n",
    "        for box in target['boxes']:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=1, edgecolor='g', facecolor='none'\n",
    "            )\n",
    "            ax[0].add_patch(rect)\n",
    "        ax[0].set_title('Ground Truth')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(denorm_img) \n",
    "        for box in prediction[0]['boxes']:\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=1, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            ax[1].add_patch(rect)\n",
    "        ax[1].set_title('Predictions')\n",
    "        ax[1].axis('off')\n",
    "        \n",
    "        # Predicciones\n",
    "        ax[2].imshow(denorm_img)\n",
    "\n",
    "        filtered_preds = custom_nms(prediction)   \n",
    "        for box in filtered_preds[0]['boxes']:\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=1, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            ax[2].add_patch(rect)\n",
    "        ax[2].set_title('Predictions')\n",
    "        ax[2].axis('off')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar disponibilidad de GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "\n",
    "# Visualizar resultados\n",
    "plot_predictions(model, test_dataset, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1kRGiw_zREE"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmsUVGS6C0O1",
    "outputId": "19608392-0ab8-4a45-acee-e65a29b97bb4"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer_phase1 = optim.Adam(model1.fc.parameters(), lr=0.001)\n",
    "optimizer_phase2 = optim.Adam(model1.fc.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrQMAKFHzREG"
   },
   "source": [
    "## Analyse training evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Phase: train with frozen layers\n",
    "for name, param in model1.named_parameters():\n",
    "    if not name.startswith('fc'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "train_history_phase1, val_history_phase1 = train_classifier(\n",
    "    model1, train_dataloader, valid_dataloader, loss_fn, optimizer_phase1, device, num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7W0PEy1d8UV"
   },
   "source": [
    "Plot loss and accuracy throughout training on train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training histories from both phases\n",
    "train_history = {\n",
    "    'loss': train_history_phase1['loss'] + train_history_phase2['loss'],\n",
    "    'acc': train_history_phase1['acc'] + train_history_phase2['acc']\n",
    "}\n",
    "\n",
    "val_history = {\n",
    "    'loss': val_history_phase1['loss'] + val_history_phase2['loss'],\n",
    "    'acc': val_history_phase1['acc'] + val_history_phase2['acc']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xr48TEVlzREH"
   },
   "outputs": [],
   "source": [
    "def plotTrainingHistory(train_history, val_history):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(train_history['loss'], label='train')\n",
    "    plt.plot(val_history['loss'], label='val')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(train_history['acc'], label='train')\n",
    "    plt.plot(val_history['acc'], label='val')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "3GfeNPc4zREI",
    "outputId": "88e735b3-b4ef-4311-bff1-7e21d4062f5d"
   },
   "outputs": [],
   "source": [
    "plotTrainingHistory(train_history, val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPZLw5cfzREI"
   },
   "source": [
    "## Test the model\n",
    "\n",
    "Evaluate the model in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtmFHipizREK",
    "outputId": "ad2e59eb-264b-4802-f363-544ad73e1601"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def test_classifier(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            _, labels_idx = labels.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels_idx.cpu().numpy())\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score (weighted): {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels\n",
    "    }\n",
    "test_results = test_classifier(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq-C6glKseuO"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "In transfer learning, we often replace the head of the model (fully-connected layers responsible for classification) to fit the task. However, these new layers are not pre-trained and thus they contain an error that is backpropagated through the pre-trained part of the network during training. We can avoid this through a training strategy that is divided into two steps:\n",
    "* Freeze the pre-trained layers of the network so that their parameters are no longer updated during training and train only the head of the model\n",
    "* Unfreeze these layers and train the network as a whole.\n",
    "\n",
    "Implement this strategy and see the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WumfC2EWIKzm"
   },
   "source": [
    "## For Group Project - Task 2: Adapt multiclass classification network to regression!\n",
    "\n",
    "Now that you have a multiclass baseline, adapt the network for the regression problem, considering the following aspects:\n",
    "* How many neurons should the last layer of a regression network have?\n",
    "* What should be the final activation function for a regression network?\n",
    "* What loss functions can be used to train a regression network?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
