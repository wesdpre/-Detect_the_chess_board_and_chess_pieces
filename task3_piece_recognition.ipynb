{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaU9_sOGzRD1"
   },
   "source": [
    "# Task 3: Piece Detection + Digital Twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dvfTDUXuzRD9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np, os, torch, random, cv2, json\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.ops as ops\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDBhCwhszREA"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "w5IQVCmOzREB"
   },
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "data_aug = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_in = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_correct(id):\n",
    "    return 0 if id == 12 else id + 1\n",
    "\n",
    "i = 0\n",
    "def label_correct_annotations(filepath):\n",
    "    anns = json.load(open(filepath))\n",
    "    for i in range(len(anns['categories'])):\n",
    "        anns['categories'][i]['id'] = label_correct(anns['categories'][i]['id'])\n",
    "    for i in range(len(anns['annotations']['pieces'])):\n",
    "        anns['annotations']['pieces'][i]['category_id'] = label_correct(anns['annotations']['pieces'][i]['category_id'])\n",
    "    \n",
    "    fp = './chessred2k/label_corrected_annotations.json'\n",
    "    with open(fp, 'w') as f:\n",
    "        json.dump(anns, f)\n",
    "\n",
    "label_correct_annotations('./chessred2k/annotations.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original DSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juTsr3Lld8UT",
    "outputId": "4f307a21-f67a-4229-8c51-ea90ec364a25"
   },
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, root_dir, partition, transform=None):\n",
    "        self.root = root_dir\n",
    "        self.anns = json.load(open(os.path.join(root_dir, 'label_corrected_annotations.json')))\n",
    "\n",
    "        self.id_to_category = {c['id']: c['name'] for c in self.anns['categories']}\n",
    "        self.category_to_id = {c['name']: c['id'] for c in self.anns['categories']}\n",
    "        self.categories = [c['name'] for c in self.anns['categories']]\n",
    "\n",
    "        # --- Step 1: Pre-process ALL annotations and identify problematic images ---\n",
    "        self.all_image_ids_from_json = []\n",
    "        self.all_file_names_from_json = []\n",
    "        for img_info in self.anns['images']:\n",
    "            self.all_file_names_from_json.append(img_info['path'])\n",
    "            self.all_image_ids_from_json.append(img_info['id'])\n",
    "        self.all_image_ids_from_json = np.asarray(self.all_image_ids_from_json) # Convert to numpy for easy indexing/masking\n",
    "\n",
    "        problematic_image_ids = set() # Store image_ids that have any malformed piece annotation\n",
    "        # This will store ONLY VALID annotations, grouped by image_id\n",
    "        self.image_annotations = {}\n",
    "\n",
    "        for ann_idx, ann in enumerate(self.anns['annotations']['pieces']):\n",
    "            # Ensure required attributes for object detection are present\n",
    "            if 'bbox' not in ann or 'category_id' not in ann:\n",
    "                #print(f\"Warning: Annotation at index {ann_idx} for image_id {ann.get('image_id', 'N/A')} is malformed (missing 'bbox' or 'category_id'). This image will be excluded.\")\n",
    "                if 'image_id' in ann:\n",
    "                    problematic_image_ids.add(ann['image_id'])\n",
    "                continue # Skip this malformed annotation\n",
    "\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.image_annotations:\n",
    "                self.image_annotations[image_id] = []\n",
    "            self.image_annotations[image_id].append(ann)\n",
    "\n",
    "\n",
    "        # --- Step 2: Filter images based on split and problematic annotations ---\n",
    "        if partition == 'train':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['train']['image_ids']).astype(int)\n",
    "        elif partition == 'valid':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['val']['image_ids']).astype(int)\n",
    "        else: # 'test'\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['test']['image_ids']).astype(int)\n",
    "\n",
    "        self.file_names = [] # Final list of file names for this split\n",
    "        self.image_ids = []  # Final list of original image IDs for this split\n",
    "\n",
    "        # Iterate through all images (which you already have indexed by self.all_image_ids_from_json)\n",
    "        for i, img_id in enumerate(self.all_image_ids_from_json):\n",
    "            # Check if image belongs to current split AND is not marked as problematic\n",
    "            if img_id in raw_split_image_ids and img_id not in problematic_image_ids:\n",
    "                # Also, ensure that the image actually has valid annotations after filtering.\n",
    "                # An image could be in the split, not problematic, but simply have no pieces.\n",
    "                # Or all its pieces were problematic, so it has no valid annotations left.\n",
    "                if img_id in self.image_annotations and len(self.image_annotations[img_id]) > 0:\n",
    "                    self.image_ids.append(img_id)\n",
    "                    self.file_names.append(self.all_file_names_from_json[i])\n",
    "                else:\n",
    "                    # Optional: Print why an image might be excluded if it's in the split but has no valid annotations\n",
    "                    print(f\"Info: Image ID {img_id} (file: {self.all_file_names_from_json[i]}) in '{partition}' split has no valid annotations after initial filtering. Excluding.\")\n",
    "\n",
    "        self.file_names = np.asarray(self.file_names)\n",
    "        self.image_ids = np.asarray(self.image_ids)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Number of {partition} images: {len(self.file_names)}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.file_names[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_image_id = self.image_ids[idx] # Get the true original ID for this image in the split\n",
    "        # Retrieve annotations using the pre-processed dictionary\n",
    "        annotations_for_image = self.image_annotations.get(original_image_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # Iterate through the (already filtered and valid) annotations for this image\n",
    "        for ann in annotations_for_image:\n",
    "            x_min, y_min, width, height = ann['bbox']\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) if len(labels) > 0 else torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        # Create the target dictionary required by Faster R-CNN\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([original_image_id])\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "train_dataset = ChessDataset('./chessred2k', 'train', data_aug)\n",
    "valid_dataset = ChessDataset('./chessred2k', 'valid', data_in)\n",
    "test_dataset = ChessDataset('./chessred2k', 'test', data_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheat DSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perspective_transform(src_pts):\n",
    "    dst_pts = np.array([\n",
    "        [OFFSET, OFFSET],\n",
    "        [512-OFFSET, OFFSET],\n",
    "        [512-OFFSET, 512-OFFSET],\n",
    "        [OFFSET, 512-OFFSET]\n",
    "    ], dtype=np.float32)\n",
    "    M = cv2.getPerspectiveTransform(np.array(src_pts, dtype=np.float32), dst_pts)\n",
    "    return M\n",
    "\n",
    "def warp_boxes(boxes, M):\n",
    "    if len(boxes) == 0:\n",
    "        return torch.zeros((0, 4), dtype=torch.float32)\n",
    "    warped = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        pts = np.array([\n",
    "            [x1, y1],\n",
    "            [x2, y1],\n",
    "            [x2, y2],\n",
    "            [x1, y2]\n",
    "        ], dtype=np.float32).reshape(-1, 1, 2)\n",
    "        warped_pts = cv2.perspectiveTransform(pts, M).reshape(-1, 2)\n",
    "        x_min, y_min = np.min(warped_pts, axis=0)\n",
    "        x_max, y_max = np.max(warped_pts, axis=0)\n",
    "        warped.append([x_min, y_min, x_max, y_max])\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET = 50\n",
    "\n",
    "class CheatDataset(Dataset):\n",
    "    def __init__(self, root_dir, partition, transform=None, corners=False, warp=False):\n",
    "        self.root = root_dir\n",
    "        self.anns = json.load(open(os.path.join(root_dir, 'label_corrected_annotations.json')))\n",
    "\n",
    "        self.id_to_category = {c['id']: c['name'] for c in self.anns['categories']}\n",
    "        self.category_to_id = {c['name']: c['id'] for c in self.anns['categories']}\n",
    "        self.categories = [c['name'] for c in self.anns['categories']]\n",
    "        self.corners_f = corners\n",
    "        self.warp_f = warp \n",
    "        if self.corners_f:\n",
    "            # Load corner annotations\n",
    "            self.corners = {}\n",
    "            for corner_data in self.anns['annotations']['corners']:\n",
    "                self.corners[corner_data['image_id']] = [\n",
    "                    corner_data['corners']['top_left'],\n",
    "                    corner_data['corners']['top_right'],\n",
    "                    corner_data['corners']['bottom_right'],\n",
    "                    corner_data['corners']['bottom_left']\n",
    "                ]\n",
    "\n",
    "        self.all_image_ids_from_json = []\n",
    "        self.all_file_names_from_json = []\n",
    "        for img_info in self.anns['images']:\n",
    "            self.all_file_names_from_json.append(img_info['path'])\n",
    "            self.all_image_ids_from_json.append(img_info['id'])\n",
    "\n",
    "        self.all_image_ids_from_json = np.asarray(self.all_image_ids_from_json)\n",
    "\n",
    "        problematic_image_ids = set()\n",
    "        self.image_annotations = {}\n",
    "\n",
    "        for ann in self.anns['annotations']['pieces']:\n",
    "            if 'bbox' not in ann or 'category_id' not in ann:\n",
    "                if 'image_id' in ann:\n",
    "                    problematic_image_ids.add(ann['image_id'])\n",
    "                continue\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.image_annotations:\n",
    "                self.image_annotations[image_id] = []\n",
    "            self.image_annotations[image_id].append(ann)\n",
    "\n",
    "        if partition == 'train':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['train']['image_ids']).astype(int)\n",
    "        elif partition == 'valid':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['val']['image_ids']).astype(int)\n",
    "        else:\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['test']['image_ids']).astype(int)\n",
    "\n",
    "        self.file_names = []\n",
    "        self.image_ids = []\n",
    "\n",
    "        for i, img_id in enumerate(self.all_image_ids_from_json):\n",
    "            if img_id in raw_split_image_ids and img_id not in problematic_image_ids:\n",
    "                if img_id in self.image_annotations and len(self.image_annotations[img_id]) > 0:\n",
    "                    if self.corners_f:\n",
    "                        if img_id in self.corners:  # only keep if corner data is available\n",
    "                            self.image_ids.append(img_id)\n",
    "                            self.file_names.append(self.all_file_names_from_json[i])\n",
    "                    else:\n",
    "                        self.image_ids.append(img_id)\n",
    "                        self.file_names.append(self.all_file_names_from_json[i])\n",
    "        self.file_names = np.asarray(self.file_names)\n",
    "        self.image_ids = np.asarray(self.image_ids)\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Number of {partition} images: {len(self.file_names)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def get_corners(self, idx):\n",
    "        if self.corners_f:\n",
    "            image_id = self.image_ids[idx]\n",
    "            corners = self.corners[image_id]\n",
    "            return corners\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.file_names[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image_id = self.image_ids[idx]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in self.image_annotations[image_id]:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        if self.corners_f and self.warp_f:\n",
    "            corners = self.corners[image_id]\n",
    "            M = get_perspective_transform(corners)\n",
    "            image = cv2.warpPerspective(image, M, (512, 512))\n",
    "            boxes = warp_boxes(boxes, M)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([image_id])\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CheatDataset('./chessred2k', 'train', data_aug, corners=True, warp=False)\n",
    "valid_dataset = CheatDataset('./chessred2k', 'valid', data_in, corners=True, warp=False)\n",
    "test_dataset = CheatDataset('./chessred2k', 'test', data_in, corners=True, warp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAYHXPnzd8UT",
    "outputId": "199a5237-a064-457b-da4f-a7184019595f"
   },
   "outputs": [],
   "source": [
    "# Device configuration (improved version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparameters (consider these adjustments)\n",
    "batch_size = 8  # Reduced from 16 for better memory management with Faster R-CNN\n",
    "num_workers = 0  # Optimal for most systems\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Enhanced collate function for object detection.\n",
    "    Handles:\n",
    "    - Empty targets\n",
    "    - Image tensor conversion\n",
    "    - Device movement\n",
    "    \"\"\"\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    processed_images = []\n",
    "    processed_targets = []\n",
    "    \n",
    "    for image, target in zip(images, targets):\n",
    "        # Convert image if not already a tensor\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Handle empty targets\n",
    "        if len(target['boxes']) == 0:\n",
    "            target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            assert target['boxes'].shape[1] == 4, \"Bounding boxes must have shape [N, 4]\"\n",
    "            \n",
    "        processed_images.append(image)\n",
    "        processed_targets.append(target)\n",
    "    \n",
    "    return processed_images, processed_targets\n",
    "\n",
    "# DataLoaders with improved settings\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Prevents partial batches\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Keep all validation samples\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Often better to use batch_size=1 for testing\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets) in enumerate(valid_dataloader):\n",
    "    print(f\"\\nBatch {i}:\")\n",
    "    for j, t in enumerate(targets):\n",
    "        print(f\"  Target {j}:\")\n",
    "        print(f\"    Boxes shape: {t['boxes'].shape}\")\n",
    "        print(f\"    Labels shape: {t['labels'].shape}\")\n",
    "        if t['boxes'].shape[0] != t['labels'].shape[0]:\n",
    "            print(\"    Inconsistency!\")\n",
    "    \n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the dataset is loaded correctly\n",
    "for i in range(len(valid_dataset)):\n",
    "    _, target = valid_dataset[i]\n",
    "    assert target['boxes'].dim() == 2 and target['boxes'].shape[1] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating dataset with pieces\n",
    "\n",
    "def plot_boxes(image, boxes, title=\"Detections\"):\n",
    "    plt.imshow(image)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso con tus datos:\n",
    "image, target = train_dataset[1]\n",
    "denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "             torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "denorm_img = np.clip(denorm_img, 0, 1)\n",
    "\n",
    "plot_boxes(denorm_img, target['boxes'], \"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aqWR_0VzRED"
   },
   "source": [
    "## Defining the model\n",
    "\n",
    "We will use a pre-trained Faster RCNN ResNet50 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfK3c9RSzRED",
    "outputId": "9cd92465-f107-4511-80a2-34f07fd16a33"
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "\n",
    "num_classes = 13  # 12 pieces + empty space\n",
    "    \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "step_size = 3 \n",
    "gamma = 0.1\n",
    "\n",
    "# Optimizador y scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=learning_rate, \n",
    "                     momentum=momentum, \n",
    "                     weight_decay=weight_decay)\n",
    "lr_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0).item()\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0).item()\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0).item()\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0).item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': total_loss/(progress_bar.n+1),\n",
    "            'Cls': loss_classifier/(progress_bar.n+1),\n",
    "            'Box': loss_box_reg/(progress_bar.n+1)\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0\n",
    "    \n",
    "    # Configuraci√≥n especial para evitar errores de dimensiones\n",
    "    original_score_thresh = model.roi_heads.score_thresh\n",
    "    model.roi_heads.score_thresh = 0.0  # Desactiva filtrado inicial\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc='Validating')):\n",
    "        # 1. Conversi√≥n expl√≠cita y verificaci√≥n\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{\n",
    "            'boxes': t['boxes'].float().to(device),\n",
    "            'labels': t['labels'].long().to(device)\n",
    "        } for t in targets]\n",
    "        \n",
    "        # 3. Forward pass con manejo de excepciones interno\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # 4. C√°lculo de p√©rdida unificado\n",
    "        if isinstance(loss_dict, dict):\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        else:  # Caso raro de lista de diccionarios\n",
    "            losses = sum(sum(l.values()) for l in loss_dict)\n",
    "        \n",
    "        total_loss += losses\n",
    "        processed_batches += 1\n",
    "                \n",
    "    \n",
    "    # Restaura configuraci√≥n original\n",
    "    model.roi_heads.score_thresh = original_score_thresh\n",
    "\n",
    "    return total_loss / processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "def train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs, model_path, early_stop_patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        train_loss = train_one_epoch(model, optimizer, train_dataloader, device, epoch)\n",
    "        \n",
    "        # Validaci√≥n\n",
    "        val_loss = evaluate(model, valid_dataloader, device)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"¬°Nuevo mejor modelo guardado! (val_loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping en epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Log de progreso\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(\"¬°Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs, '13classes_warped.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, 13)\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load('13classes_base.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nms(predictions, iou_threshold=0.5):\n",
    "    filtered_preds = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        boxes = pred['boxes']\n",
    "        scores = pred['scores']\n",
    "        labels = pred['labels']\n",
    "\n",
    "        if boxes.numel() == 0:\n",
    "            filtered_preds.append(pred)\n",
    "            continue\n",
    "\n",
    "        # Apply NMS with no pre-filtering\n",
    "        keep_idxs = ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "        # Keep only those selected by NMS\n",
    "        filtered_preds.append({\n",
    "            'boxes': boxes[keep_idxs],\n",
    "            'scores': scores[keep_idxs],\n",
    "            'labels': labels[keep_idxs]\n",
    "        })\n",
    "\n",
    "    return filtered_preds\n",
    "\n",
    "def plot_prediction(model, image, target, debug=False):\n",
    "    img_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "    \n",
    "    # Procesamiento de la imagen para visualizaci√≥n\n",
    "    denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "                    torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "    denorm_img = np.clip(denorm_img, 0, 1)\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 7))\n",
    "    \n",
    "    # Ground Truth\n",
    "    ax[0].imshow(denorm_img)\n",
    "    for box in target['boxes']:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=1, edgecolor='g', facecolor='none'\n",
    "        )\n",
    "        ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground Truth')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(denorm_img) \n",
    "    for box in prediction[0]['boxes']:\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=1, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Predictions')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Predicciones\n",
    "    ax[2].imshow(denorm_img)\n",
    "\n",
    "    filtered_preds = custom_nms(prediction)   \n",
    "    for box in filtered_preds[0]['boxes']:\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=1, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax[2].add_patch(rect)\n",
    "    ax[2].set_title('Predictions')\n",
    "    ax[2].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(model, dataset, num_samples=3):\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, target = dataset[idx]\n",
    "        plot_prediction(model, image, target)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(model, test_dataset, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, device, data_loader, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    total_mse = 0.0\n",
    "    matched_count = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc='Testing'):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{\n",
    "                'boxes': t['boxes'].float().to(device),\n",
    "                'labels': t['labels'].long().to(device)\n",
    "            } for t in targets]\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            for pred, gt in zip(predictions, targets):\n",
    "                pred_boxes = pred['boxes']\n",
    "                pred_labels = pred['labels']\n",
    "                pred_scores = pred['scores']\n",
    "\n",
    "                gt_boxes = gt['boxes']\n",
    "                gt_labels = gt['labels']\n",
    "\n",
    "                if pred_boxes.numel() == 0:\n",
    "                    # No predictions ‚Äî all GTs are false negatives\n",
    "                    all_targets.extend(gt_labels.cpu().tolist())\n",
    "                    all_preds.extend([-1] * len(gt_labels))  # -1 = no prediction\n",
    "                    total_fn += len(gt_labels)\n",
    "                    continue\n",
    "\n",
    "                if gt_boxes.numel() == 0:\n",
    "                    # No GT ‚Äî all predictions are false positives\n",
    "                    all_targets.extend([-1] * len(pred_labels))\n",
    "                    all_preds.extend(pred_labels.cpu().tolist())\n",
    "                    total_fp += len(pred_labels)\n",
    "                    continue\n",
    "\n",
    "                ious = ops.box_iou(gt_boxes, pred_boxes)  # [num_gt, num_pred]\n",
    "                gt_matched = set()\n",
    "                pred_matched = set()\n",
    "\n",
    "                for gt_idx in range(len(gt_boxes)):\n",
    "                    iou_row = ious[gt_idx]\n",
    "                    max_iou, pred_idx = torch.max(iou_row, dim=0)\n",
    "\n",
    "                    if max_iou >= iou_threshold and pred_idx.item() not in pred_matched:\n",
    "                        # Match found\n",
    "                        gt_label = gt_labels[gt_idx].item()\n",
    "                        pred_label = pred_labels[pred_idx].item()\n",
    "\n",
    "                        all_targets.append(gt_label)\n",
    "                        all_preds.append(pred_label)\n",
    "\n",
    "                        # Bounding box MSE\n",
    "                        mse = F.mse_loss(\n",
    "                            pred_boxes[pred_idx],\n",
    "                            gt_boxes[gt_idx],\n",
    "                            reduction='mean'\n",
    "                        ).item()\n",
    "                        total_mse += mse\n",
    "                        matched_count += 1\n",
    "\n",
    "                        gt_matched.add(gt_idx)\n",
    "                        pred_matched.add(pred_idx.item())\n",
    "                    else:\n",
    "                        # No good match found\n",
    "                        continue\n",
    "\n",
    "                # Count false negatives (GTs not matched)\n",
    "                for gt_idx in range(len(gt_boxes)):\n",
    "                    if gt_idx not in gt_matched:\n",
    "                        all_targets.append(gt_labels[gt_idx].item())\n",
    "                        all_preds.append(-1)\n",
    "                        total_fn += 1\n",
    "\n",
    "                # Count false positives (preds not matched)\n",
    "                for pred_idx in range(len(pred_boxes)):\n",
    "                    if pred_idx not in pred_matched:\n",
    "                        all_targets.append(-1)\n",
    "                        all_preds.append(pred_labels[pred_idx].item())\n",
    "                        total_fp += 1\n",
    "\n",
    "    # Filter out unmatched (-1) before computing classification metrics\n",
    "    y_true = torch.tensor([t for t in all_targets if t != -1])\n",
    "    y_pred = torch.tensor([p for t, p in zip(all_targets, all_preds) if t != -1])\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    avg_mse = total_mse / matched_count if matched_count > 0 else float('inf')\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"MSE (boxes): {avg_mse:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Board extraction preprocessing\n",
    "test_model(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With\n",
    "test_model(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digital Twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_board_matrix(boxes, labels):\n",
    "    \"\"\"\n",
    "    Convert predicted bounding boxes and labels into an 8x8 board matrix.\n",
    "    \n",
    "    Parameters:\n",
    "        boxes (Tensor or list of list): Nx4 format [[x1, y1, x2, y2], ...]\n",
    "        labels (Tensor or list): length-N label list\n",
    "        \n",
    "    Returns:\n",
    "        board_matrix: 8x8 numpy array with labels placed in appropriate squares\n",
    "    \"\"\"\n",
    "    board = np.zeros((8, 8), dtype=int)\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        \n",
    "        col = int(cx // 64)\n",
    "        row = int(cy // 64)\n",
    "        \n",
    "        # Clamp to ensure we don't go out of bounds due to rounding\n",
    "        col = max(0, min(7, col))\n",
    "        row = max(0, min(7, row))\n",
    "        \n",
    "        board[row, col] = int(label)  # Fill square with label\n",
    "    \n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_board_matrix(boxes, labels, offset=20, board_size=512, squares=8):\n",
    "    \"\"\"\n",
    "    Convert predicted bounding boxes and labels into an 8x8 board matrix,\n",
    "    adjusting for homography offset padding.\n",
    "    \n",
    "    Parameters:\n",
    "        boxes (Tensor or list of list): Nx4 format [[x1, y1, x2, y2], ...]\n",
    "        labels (Tensor or list): length-N label list\n",
    "        offset (int): pixel offset added during warp\n",
    "        board_size (int): total size of the board image (typically 512)\n",
    "        squares (int): number of squares per row/column (8 for standard chess)\n",
    "\n",
    "    Returns:\n",
    "        board_matrix: 8x8 numpy array with labels placed in appropriate squares\n",
    "    \"\"\"\n",
    "    board = np.zeros((squares, squares), dtype=int)\n",
    "    square_size = (board_size - 2 * offset) / squares\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        cx = ((x1 + x2) / 2) - offset\n",
    "        cy = ((y1 + y2) / 2) - offset\n",
    "\n",
    "        col = int(cx // square_size)\n",
    "        row = int(cy // square_size)\n",
    "\n",
    "        # Clamp to stay within bounds\n",
    "        col = max(0, min(squares - 1, col))\n",
    "        row = max(0, min(squares - 1, row))\n",
    "\n",
    "        board[row, col] = int(label)\n",
    "\n",
    "    return board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "# === Load image and targets ===\n",
    "img, targets = test_dataset[4]\n",
    "\n",
    "# === Denormalize image ===\n",
    "denorm_img = (img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "             torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "denorm_img = np.clip(denorm_img, 0, 1)\n",
    "\n",
    "# === Get perspective transform ===\n",
    "corners = test_dataset.get_corners(4)\n",
    "M = get_perspective_transform(corners)\n",
    "\n",
    "# === Warp image ===\n",
    "warp_img = cv2.warpPerspective((denorm_img * 255).astype(np.uint8), M, (512, 512))\n",
    "\n",
    "# === Run model prediction ===\n",
    "img_tensor = img.unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(img_tensor)[0]\n",
    "\n",
    "# === Optionally filter predictions (custom NMS or confidence threshold) ===\n",
    "prediction = custom_nms([prediction])[0]  # if using custom filtering\n",
    "\n",
    "# === Warp predicted boxes ===\n",
    "boxes = prediction['boxes'].cpu().numpy()\n",
    "labels = prediction['labels'].cpu().numpy()\n",
    "\n",
    "warped_boxes = warp_boxes(boxes, M)\n",
    "\n",
    "# === Plot warped image and boxes ===\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(warp_img)\n",
    "\n",
    "for box, label in zip(warped_boxes, labels):\n",
    "    x1, y1, x2, y2 = box\n",
    "    rect = Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                     linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1 - 5, str(label), color='yellow', fontsize=8, backgroundcolor='black')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_to_board_matrix(warped_boxes, prediction['labels'], offset=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.152 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.147 üöÄ Python-3.13.2 torch-2.7.0 CPU (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=yolo_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo11n_run2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=chess_board_detection, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=chess_board_detection/yolo11n_run2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=13\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    433207  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,592,375 parameters, 2,592,359 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1865.0¬±264.3 MB/s, size: 2197.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/santiagoromero/Documents/cv/-Detect_the_chess_board_and_chess_pieces/yolo_dataset/train/labels... 1442 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1442/1442 [00:00<00:00, 6133.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/santiagoromero/Documents/cv/-Detect_the_chess_board_and_chess_pieces/yolo_dataset/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1784.2¬±101.8 MB/s, size: 1766.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/santiagoromero/Documents/cv/-Detect_the_chess_board_and_chess_pieces/yolo_dataset/val/labels... 330 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330/330 [00:00<00:00, 5998.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/santiagoromero/Documents/cv/-Detect_the_chess_board_and_chess_pieces/yolo_dataset/val/labels.cache\n",
      "Plotting labels to chess_board_detection/yolo11n_run2/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mchess_board_detection/yolo11n_run2\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100         0G     0.9973      3.438     0.9078         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:11<00:00,  1.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.407      0.376      0.354      0.276\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      2/100         0G     0.7377      1.476     0.8416         84        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:02<00:00,  1.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.577      0.692       0.67      0.546\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      3/100         0G     0.6442     0.9983     0.8269         67        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:02<00:00,  1.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.752      0.828      0.844      0.648\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      4/100         0G     0.5915     0.8468     0.8187         41        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:00<00:00,  1.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.869      0.892      0.933      0.787\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      5/100         0G     0.5453     0.7378     0.8132         93        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [04:57<00:00,  1.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.939       0.93      0.969      0.819\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      6/100         0G     0.5314     0.6555     0.8126         46        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [04:58<00:00,  1.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.957      0.961      0.984      0.815\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      7/100         0G     0.5094     0.5999     0.8115         34        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [04:59<00:00,  1.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.972      0.968      0.988      0.823\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      8/100         0G     0.4873     0.5635     0.8064         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:00<00:00,  1.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132       0.98      0.964      0.991      0.834\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      9/100         0G     0.4947     0.5605     0.8069         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [04:57<00:00,  1.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:53<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.969      0.953      0.988      0.864\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     10/100         0G     0.4894     0.5234     0.8063         33        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [04:56<00:00,  1.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132       0.98      0.977      0.992      0.856\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     11/100         0G     0.4628     0.4894     0.8041         57        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:14<00:00,  1.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:51<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.984      0.978      0.993      0.832\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     12/100         0G     0.4588     0.4768      0.804        116        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:17<00:00,  1.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:51<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.986       0.98      0.993      0.862\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     13/100         0G     0.4473     0.4628     0.8034         45        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:28<00:00,  1.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:51<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.985      0.983      0.993      0.844\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     14/100         0G     0.4436     0.4463     0.8007         63        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:29<00:00,  1.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:51<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.989      0.985      0.994      0.868\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     15/100         0G     0.4518     0.4431     0.7996         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:32<00:00,  1.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:52<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.991      0.979      0.994      0.857\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     16/100         0G     0.4327     0.4269     0.7989         49        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:27<00:00,  1.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:51<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.989      0.988      0.994      0.843\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     17/100         0G     0.4412      0.432     0.7986        123        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:26<00:00,  1.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:51<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.984      0.988      0.994      0.853\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     18/100         0G     0.4331     0.4168     0.7988         70        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [05:27<00:00,  1.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:51<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132       0.99      0.987      0.994      0.857\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     19/100         0G     0.4278     0.4013     0.7964         88        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [06:24<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [15:49<00:00, 45.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.994      0.987      0.995      0.835\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     20/100         0G     0.4239     0.3984     0.7973         40        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [39:58<00:00, 13.25s/it]   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:56<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.988       0.99      0.995       0.86\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     21/100         0G     0.4091     0.3879     0.7956        125        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [07:23<00:00,  2.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:14<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.995      0.988      0.995      0.847\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     22/100         0G     0.4078     0.3791      0.796         58        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [07:38<00:00,  2.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:17<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.993      0.987      0.994      0.863\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     23/100         0G     0.4168     0.3804     0.7971        101        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [07:50<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:22<00:00,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.993      0.989      0.995      0.845\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     24/100         0G     0.4045     0.3719     0.7951         67        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181/181 [07:46<00:00,  2.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:21<00:00,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.993       0.99      0.995      0.839\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 14, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "24 epochs completed in 3.472 hours.\n",
      "Optimizer stripped from chess_board_detection/yolo11n_run2/weights/last.pt, 5.5MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from chess_board_detection/yolo11n_run2/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating chess_board_detection/yolo11n_run2/weights/best.pt...\n",
      "Ultralytics 8.3.147 üöÄ Python-3.13.2 torch-2.7.0 CPU (Apple M4)\n",
      "YOLO11n summary (fused): 100 layers, 2,584,687 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:15<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        330       6132      0.989      0.985      0.994      0.868\n",
      "            white-pawn        330       1625      0.999      0.994      0.995      0.858\n",
      "            white-rook        281        447      0.998      0.994      0.995      0.848\n",
      "          white-knight        220        274          1      0.991      0.995      0.861\n",
      "          white-bishop        235        335      0.997      0.991      0.995       0.86\n",
      "           white-queen        126        126      0.984      0.986      0.993        0.9\n",
      "            white-king        330        330      0.995      0.991      0.995       0.91\n",
      "            black-pawn        330       1511      0.991      0.994      0.995      0.862\n",
      "            black-rook        280        471          1      0.992      0.995      0.874\n",
      "          black-knight        125        178      0.978      0.972      0.992      0.825\n",
      "          black-bishop        223        380      0.992      0.952      0.993      0.849\n",
      "           black-queen        125        125      0.934      0.984      0.986      0.867\n",
      "            black-king        330        330      0.997      0.975      0.995        0.9\n",
      "Speed: 0.9ms preprocess, 177.6ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mchess_board_detection/yolo11n_run2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11n.pt\") \n",
    "\n",
    "results = model.train(data=\"yolo_dataset/dataset.yaml\", \n",
    "                      epochs=100, imgsz=640, device=\"cpu\", \n",
    "                      batch=8, project=\"chess_board_detection\", \n",
    "                      name=\"yolo11n_run\",\n",
    "                      patience=10,\n",
    "                      pretrained=True )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
