{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaU9_sOGzRD1"
   },
   "source": [
    "# Task 3: Piece Detection + Digital Twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvfTDUXuzRD9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np, os, torch, random, cv2, json\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.ops as ops\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, mean_squared_error\n",
    "\n",
    "from src.our_chessboard_detection import read_image, apply_filters, get_contours, rotate_and_crop, align_board\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDBhCwhszREA"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5IQVCmOzREB"
   },
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "data_aug = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_in = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_correct(id):\n",
    "    return 0 if id == 12 else id + 1\n",
    "\n",
    "i = 0\n",
    "def label_correct_annotations(filepath):\n",
    "    anns = json.load(open(filepath))\n",
    "    for i in range(len(anns['categories'])):\n",
    "        anns['categories'][i]['id'] = label_correct(anns['categories'][i]['id'])\n",
    "    for i in range(len(anns['annotations']['pieces'])):\n",
    "        anns['annotations']['pieces'][i]['category_id'] = label_correct(anns['annotations']['pieces'][i]['category_id'])\n",
    "    \n",
    "    fp = './chessred2k/label_corrected_annotations.json'\n",
    "    with open(fp, 'w') as f:\n",
    "        json.dump(anns, f)\n",
    "\n",
    "label_correct_annotations('./annotations.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original DSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juTsr3Lld8UT",
    "outputId": "4f307a21-f67a-4229-8c51-ea90ec364a25"
   },
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, root_dir, partition, transform=None):\n",
    "        self.root = root_dir\n",
    "        self.anns = json.load(open(os.path.join(root_dir, 'label_corrected_annotations.json')))\n",
    "\n",
    "        self.id_to_category = {c['id']: c['name'] for c in self.anns['categories']}\n",
    "        self.category_to_id = {c['name']: c['id'] for c in self.anns['categories']}\n",
    "        self.categories = [c['name'] for c in self.anns['categories']]\n",
    "\n",
    "        # --- Step 1: Pre-process ALL annotations and identify problematic images ---\n",
    "        self.all_image_ids_from_json = []\n",
    "        self.all_file_names_from_json = []\n",
    "        for img_info in self.anns['images']:\n",
    "            self.all_file_names_from_json.append(img_info['path'])\n",
    "            self.all_image_ids_from_json.append(img_info['id'])\n",
    "        self.all_image_ids_from_json = np.asarray(self.all_image_ids_from_json) # Convert to numpy for easy indexing/masking\n",
    "\n",
    "        problematic_image_ids = set() # Store image_ids that have any malformed piece annotation\n",
    "        # This will store ONLY VALID annotations, grouped by image_id\n",
    "        self.image_annotations = {}\n",
    "\n",
    "        for ann_idx, ann in enumerate(self.anns['annotations']['pieces']):\n",
    "            # Ensure required attributes for object detection are present\n",
    "            if 'bbox' not in ann or 'category_id' not in ann:\n",
    "                #print(f\"Warning: Annotation at index {ann_idx} for image_id {ann.get('image_id', 'N/A')} is malformed (missing 'bbox' or 'category_id'). This image will be excluded.\")\n",
    "                if 'image_id' in ann:\n",
    "                    problematic_image_ids.add(ann['image_id'])\n",
    "                continue # Skip this malformed annotation\n",
    "\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.image_annotations:\n",
    "                self.image_annotations[image_id] = []\n",
    "            self.image_annotations[image_id].append(ann)\n",
    "\n",
    "\n",
    "        # --- Step 2: Filter images based on split and problematic annotations ---\n",
    "        if partition == 'train':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['train']['image_ids']).astype(int)\n",
    "        elif partition == 'valid':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['val']['image_ids']).astype(int)\n",
    "        else: # 'test'\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['test']['image_ids']).astype(int)\n",
    "\n",
    "        self.file_names = [] # Final list of file names for this split\n",
    "        self.image_ids = []  # Final list of original image IDs for this split\n",
    "\n",
    "        # Iterate through all images (which you already have indexed by self.all_image_ids_from_json)\n",
    "        for i, img_id in enumerate(self.all_image_ids_from_json):\n",
    "            # Check if image belongs to current split AND is not marked as problematic\n",
    "            if img_id in raw_split_image_ids and img_id not in problematic_image_ids:\n",
    "                # Also, ensure that the image actually has valid annotations after filtering.\n",
    "                # An image could be in the split, not problematic, but simply have no pieces.\n",
    "                # Or all its pieces were problematic, so it has no valid annotations left.\n",
    "                if img_id in self.image_annotations and len(self.image_annotations[img_id]) > 0:\n",
    "                    self.image_ids.append(img_id)\n",
    "                    self.file_names.append(self.all_file_names_from_json[i])\n",
    "                else:\n",
    "                    # Optional: Print why an image might be excluded if it's in the split but has no valid annotations\n",
    "                    print(f\"Info: Image ID {img_id} (file: {self.all_file_names_from_json[i]}) in '{partition}' split has no valid annotations after initial filtering. Excluding.\")\n",
    "\n",
    "        self.file_names = np.asarray(self.file_names)\n",
    "        self.image_ids = np.asarray(self.image_ids)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Number of {partition} images: {len(self.file_names)}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.file_names[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_image_id = self.image_ids[idx] # Get the true original ID for this image in the split\n",
    "        # Retrieve annotations using the pre-processed dictionary\n",
    "        annotations_for_image = self.image_annotations.get(original_image_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # Iterate through the (already filtered and valid) annotations for this image\n",
    "        for ann in annotations_for_image:\n",
    "            x_min, y_min, width, height = ann['bbox']\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) if len(labels) > 0 else torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        # Create the target dictionary required by Faster R-CNN\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([original_image_id])\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "train_dataset = ChessDataset('./chessred2k', 'train', data_aug)\n",
    "valid_dataset = ChessDataset('./chessred2k', 'valid', data_in)\n",
    "test_dataset = ChessDataset('./chessred2k', 'test', data_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheat DSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perspective_transform(src_pts):\n",
    "    dst_pts = np.array([\n",
    "        [OFFSET, OFFSET],\n",
    "        [512-OFFSET, OFFSET],\n",
    "        [512-OFFSET, 512-OFFSET],\n",
    "        [OFFSET, 512-OFFSET]\n",
    "    ], dtype=np.float32)\n",
    "    M = cv2.getPerspectiveTransform(np.array(src_pts, dtype=np.float32), dst_pts)\n",
    "    return M\n",
    "\n",
    "def warp_boxes(boxes, M):\n",
    "    if len(boxes) == 0:\n",
    "        return torch.zeros((0, 4), dtype=torch.float32)\n",
    "    warped = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        pts = np.array([\n",
    "            [x1, y1],\n",
    "            [x2, y1],\n",
    "            [x2, y2],\n",
    "            [x1, y2]\n",
    "        ], dtype=np.float32).reshape(-1, 1, 2)\n",
    "        warped_pts = cv2.perspectiveTransform(pts, M).reshape(-1, 2)\n",
    "        x_min, y_min = np.min(warped_pts, axis=0)\n",
    "        x_max, y_max = np.max(warped_pts, axis=0)\n",
    "        warped.append([x_min, y_min, x_max, y_max])\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET = 50\n",
    "\n",
    "class CheatDataset(Dataset):\n",
    "    def __init__(self, root_dir, partition, transform=None, corners=False, warp=False, annotations_file='label_corrected_annotations.json'):\n",
    "        self.root = root_dir\n",
    "        self.anns = json.load(open(os.path.join(root_dir, annotations_file)))\n",
    "\n",
    "        self.id_to_category = {c['id']: c['name'] for c in self.anns['categories']}\n",
    "        self.category_to_id = {c['name']: c['id'] for c in self.anns['categories']}\n",
    "        self.categories = [c['name'] for c in self.anns['categories']]\n",
    "        self.corners_f = corners\n",
    "        self.warp_f = warp \n",
    "        if self.corners_f:\n",
    "            # Load corner annotations\n",
    "            self.corners = {}\n",
    "            for corner_data in self.anns['annotations']['corners']:\n",
    "                self.corners[corner_data['image_id']] = [\n",
    "                    corner_data['corners']['top_left'],\n",
    "                    corner_data['corners']['top_right'],\n",
    "                    corner_data['corners']['bottom_right'],\n",
    "                    corner_data['corners']['bottom_left']\n",
    "                ]\n",
    "\n",
    "        self.all_image_ids_from_json = []\n",
    "        self.all_file_names_from_json = []\n",
    "        for img_info in self.anns['images']:\n",
    "            self.all_file_names_from_json.append(img_info['path'])\n",
    "            self.all_image_ids_from_json.append(img_info['id'])\n",
    "\n",
    "        self.all_image_ids_from_json = np.asarray(self.all_image_ids_from_json)\n",
    "\n",
    "        problematic_image_ids = set()\n",
    "        self.image_annotations = {}\n",
    "\n",
    "        for ann in self.anns['annotations']['pieces']:\n",
    "            if 'bbox' not in ann or 'category_id' not in ann:\n",
    "                if 'image_id' in ann:\n",
    "                    problematic_image_ids.add(ann['image_id'])\n",
    "                continue\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.image_annotations:\n",
    "                self.image_annotations[image_id] = []\n",
    "            self.image_annotations[image_id].append(ann)\n",
    "\n",
    "        if partition == 'train':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['train']['image_ids']).astype(int)\n",
    "        elif partition == 'valid':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['val']['image_ids']).astype(int)\n",
    "        else:\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['test']['image_ids']).astype(int)\n",
    "\n",
    "        self.file_names = []\n",
    "        self.image_ids = []\n",
    "\n",
    "        for i, img_id in enumerate(self.all_image_ids_from_json):\n",
    "            if img_id in raw_split_image_ids and img_id not in problematic_image_ids:\n",
    "                if img_id in self.image_annotations and len(self.image_annotations[img_id]) > 0:\n",
    "                    if self.corners_f:\n",
    "                        if img_id in self.corners:  # only keep if corner data is available\n",
    "                            self.image_ids.append(img_id)\n",
    "                            self.file_names.append(self.all_file_names_from_json[i])\n",
    "                    else:\n",
    "                        self.image_ids.append(img_id)\n",
    "                        self.file_names.append(self.all_file_names_from_json[i])\n",
    "        self.file_names = np.asarray(self.file_names)\n",
    "        self.image_ids = np.asarray(self.image_ids)\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Number of {partition} images: {len(self.file_names)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def get_corners(self, idx):\n",
    "        if self.corners_f:\n",
    "            image_id = self.image_ids[idx]\n",
    "            corners = self.corners[image_id]\n",
    "            return corners\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.file_names[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image_id = self.image_ids[idx]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in self.image_annotations[image_id]:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        if self.corners_f and self.warp_f:\n",
    "            corners = self.corners[image_id]\n",
    "            M = get_perspective_transform(corners)\n",
    "            image = cv2.warpPerspective(image, M, (512, 512))\n",
    "            boxes = warp_boxes(boxes, M)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([image_id])\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CheatDataset('', 'train', data_aug, corners=True, warp=False, annotations_file='annotations.json')\n",
    "valid_dataset = CheatDataset('', 'valid', data_in, corners=True, warp=False, annotations_file='annotations.json')\n",
    "test_dataset = CheatDataset('', 'test', data_in, corners=True, warp=False, annotations_file='annotations.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAYHXPnzd8UT",
    "outputId": "199a5237-a064-457b-da4f-a7184019595f"
   },
   "outputs": [],
   "source": [
    "# Device configuration (improved version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparameters (consider these adjustments)\n",
    "batch_size = 8  # Reduced from 16 for better memory management with Faster R-CNN\n",
    "num_workers = 0  # Optimal for most systems\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Enhanced collate function for object detection.\n",
    "    Handles:\n",
    "    - Empty targets\n",
    "    - Image tensor conversion\n",
    "    - Device movement\n",
    "    \"\"\"\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    processed_images = []\n",
    "    processed_targets = []\n",
    "    \n",
    "    for image, target in zip(images, targets):\n",
    "        # Convert image if not already a tensor\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Handle empty targets\n",
    "        if len(target['boxes']) == 0:\n",
    "            target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            assert target['boxes'].shape[1] == 4, \"Bounding boxes must have shape [N, 4]\"\n",
    "            \n",
    "        processed_images.append(image)\n",
    "        processed_targets.append(target)\n",
    "    \n",
    "    return processed_images, processed_targets\n",
    "\n",
    "# DataLoaders with improved settings\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Prevents partial batches\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Keep all validation samples\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Often better to use batch_size=1 for testing\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets) in enumerate(valid_dataloader):\n",
    "    print(f\"\\nBatch {i}:\")\n",
    "    for j, t in enumerate(targets):\n",
    "        print(f\"  Target {j}:\")\n",
    "        print(f\"    Boxes shape: {t['boxes'].shape}\")\n",
    "        print(f\"    Labels shape: {t['labels'].shape}\")\n",
    "        if t['boxes'].shape[0] != t['labels'].shape[0]:\n",
    "            print(\"    Inconsistency!\")\n",
    "    \n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the dataset is loaded correctly\n",
    "for i in range(len(valid_dataset)):\n",
    "    _, target = valid_dataset[i]\n",
    "    assert target['boxes'].dim() == 2 and target['boxes'].shape[1] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating dataset with pieces\n",
    "\n",
    "def plot_boxes(image, boxes, title=\"Detections\"):\n",
    "    plt.imshow(image)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso con tus datos:\n",
    "image, target = train_dataset[1]\n",
    "denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "             torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "denorm_img = np.clip(denorm_img, 0, 1)\n",
    "\n",
    "plot_boxes(denorm_img, target['boxes'], \"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aqWR_0VzRED"
   },
   "source": [
    "# FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfK3c9RSzRED",
    "outputId": "9cd92465-f107-4511-80a2-34f07fd16a33"
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "\n",
    "num_classes = 13  # 12 pieces + empty space\n",
    "    \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "step_size = 3 \n",
    "gamma = 0.1\n",
    "\n",
    "# Optimizador y scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=learning_rate, \n",
    "                     momentum=momentum, \n",
    "                     weight_decay=weight_decay)\n",
    "lr_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0).item()\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0).item()\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0).item()\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0).item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': total_loss/(progress_bar.n+1),\n",
    "            'Cls': loss_classifier/(progress_bar.n+1),\n",
    "            'Box': loss_box_reg/(progress_bar.n+1)\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0\n",
    "    \n",
    "    # Configuración especial para evitar errores de dimensiones\n",
    "    original_score_thresh = model.roi_heads.score_thresh\n",
    "    model.roi_heads.score_thresh = 0.0  # Desactiva filtrado inicial\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc='Validating')):\n",
    "        # 1. Conversión explícita y verificación\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{\n",
    "            'boxes': t['boxes'].float().to(device),\n",
    "            'labels': t['labels'].long().to(device)\n",
    "        } for t in targets]\n",
    "        \n",
    "        # 3. Forward pass con manejo de excepciones interno\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # 4. Cálculo de pérdida unificado\n",
    "        if isinstance(loss_dict, dict):\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        else:  # Caso raro de lista de diccionarios\n",
    "            losses = sum(sum(l.values()) for l in loss_dict)\n",
    "        \n",
    "        total_loss += losses\n",
    "        processed_batches += 1\n",
    "                \n",
    "    \n",
    "    # Restaura configuración original\n",
    "    model.roi_heads.score_thresh = original_score_thresh\n",
    "\n",
    "    return total_loss / processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "def train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs, model_path, early_stop_patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        train_loss = train_one_epoch(model, optimizer, train_dataloader, device, epoch)\n",
    "        \n",
    "        # Validación\n",
    "        val_loss = evaluate(model, valid_dataloader, device)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"¡Nuevo mejor modelo guardado! (val_loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping en epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Log de progreso\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(\"¡Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs, '13classes_warped.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, 13)\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load('13classes_base.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nms(predictions, iou_threshold=0.5):\n",
    "    filtered_preds = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        boxes = pred['boxes']\n",
    "        scores = pred['scores']\n",
    "        labels = pred['labels']\n",
    "\n",
    "        if boxes.numel() == 0:\n",
    "            filtered_preds.append(pred)\n",
    "            continue\n",
    "\n",
    "        # Apply NMS with no pre-filtering\n",
    "        keep_idxs = ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "        # Keep only those selected by NMS\n",
    "        filtered_preds.append({\n",
    "            'boxes': boxes[keep_idxs],\n",
    "            'scores': scores[keep_idxs],\n",
    "            'labels': labels[keep_idxs]\n",
    "        })\n",
    "\n",
    "    return filtered_preds\n",
    "\n",
    "def plot_prediction(model, image, target, debug=False):\n",
    "    img_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "    \n",
    "    # Procesamiento de la imagen para visualización\n",
    "    denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "                    torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "    denorm_img = np.clip(denorm_img, 0, 1)\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 7))\n",
    "    \n",
    "    # Ground Truth\n",
    "    ax[0].imshow(denorm_img)\n",
    "    for box in target['boxes']:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=1, edgecolor='g', facecolor='none'\n",
    "        )\n",
    "        ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground Truth')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(denorm_img) \n",
    "    for box in prediction[0]['boxes']:\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=1, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Predictions')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Predicciones\n",
    "    ax[2].imshow(denorm_img)\n",
    "\n",
    "    filtered_preds = custom_nms(prediction)   \n",
    "    for box in filtered_preds[0]['boxes']:\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=1, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax[2].add_patch(rect)\n",
    "    ax[2].set_title('Predictions')\n",
    "    ax[2].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(model, dataset, num_samples=3):\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, target = dataset[idx]\n",
    "        plot_prediction(model, image, target)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(model, test_dataset, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, device, data_loader, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    total_mse = 0.0\n",
    "    matched_count = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc='Testing'):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{\n",
    "                'boxes': t['boxes'].float().to(device),\n",
    "                'labels': t['labels'].long().to(device)\n",
    "            } for t in targets]\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            for pred, gt in zip(predictions, targets):\n",
    "                pred_boxes = pred['boxes']\n",
    "                pred_labels = pred['labels']\n",
    "                pred_scores = pred['scores']\n",
    "\n",
    "                gt_boxes = gt['boxes']\n",
    "                gt_labels = gt['labels']\n",
    "\n",
    "                if pred_boxes.numel() == 0:\n",
    "                    # No predictions — all GTs are false negatives\n",
    "                    all_targets.extend(gt_labels.cpu().tolist())\n",
    "                    all_preds.extend([-1] * len(gt_labels))  # -1 = no prediction\n",
    "                    total_fn += len(gt_labels)\n",
    "                    continue\n",
    "\n",
    "                if gt_boxes.numel() == 0:\n",
    "                    # No GT — all predictions are false positives\n",
    "                    all_targets.extend([-1] * len(pred_labels))\n",
    "                    all_preds.extend(pred_labels.cpu().tolist())\n",
    "                    total_fp += len(pred_labels)\n",
    "                    continue\n",
    "\n",
    "                ious = ops.box_iou(gt_boxes, pred_boxes)  # [num_gt, num_pred]\n",
    "                gt_matched = set()\n",
    "                pred_matched = set()\n",
    "\n",
    "                for gt_idx in range(len(gt_boxes)):\n",
    "                    iou_row = ious[gt_idx]\n",
    "                    max_iou, pred_idx = torch.max(iou_row, dim=0)\n",
    "\n",
    "                    if max_iou >= iou_threshold and pred_idx.item() not in pred_matched:\n",
    "                        # Match found\n",
    "                        gt_label = gt_labels[gt_idx].item()\n",
    "                        pred_label = pred_labels[pred_idx].item()\n",
    "\n",
    "                        all_targets.append(gt_label)\n",
    "                        all_preds.append(pred_label)\n",
    "\n",
    "                        # Bounding box MSE\n",
    "                        mse = F.mse_loss(\n",
    "                            pred_boxes[pred_idx],\n",
    "                            gt_boxes[gt_idx],\n",
    "                            reduction='mean'\n",
    "                        ).item()\n",
    "                        total_mse += mse\n",
    "                        matched_count += 1\n",
    "\n",
    "                        gt_matched.add(gt_idx)\n",
    "                        pred_matched.add(pred_idx.item())\n",
    "                    else:\n",
    "                        # No good match found\n",
    "                        continue\n",
    "\n",
    "                # Count false negatives (GTs not matched)\n",
    "                for gt_idx in range(len(gt_boxes)):\n",
    "                    if gt_idx not in gt_matched:\n",
    "                        all_targets.append(gt_labels[gt_idx].item())\n",
    "                        all_preds.append(-1)\n",
    "                        total_fn += 1\n",
    "\n",
    "                # Count false positives (preds not matched)\n",
    "                for pred_idx in range(len(pred_boxes)):\n",
    "                    if pred_idx not in pred_matched:\n",
    "                        all_targets.append(-1)\n",
    "                        all_preds.append(pred_labels[pred_idx].item())\n",
    "                        total_fp += 1\n",
    "\n",
    "    # Filter out unmatched (-1) before computing classification metrics\n",
    "    y_true = torch.tensor([t for t in all_targets if t != -1])\n",
    "    y_pred = torch.tensor([p for t, p in zip(all_targets, all_preds) if t != -1])\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    avg_mse = total_mse / matched_count if matched_count > 0 else float('inf')\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"MSE (boxes): {avg_mse:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Board extraction preprocessing\n",
    "test_model(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With\n",
    "test_model(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocesing dataset to turn it into YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo_dataset(\n",
    "    root_dir,\n",
    "    output_dir='yolo_dataset'\n",
    "):\n",
    "    # load anotation\n",
    "    annotations_path = os.path.join(root_dir, 'annotations.json')\n",
    "    with open(annotations_path) as f:\n",
    "        anns = json.load(f)\n",
    "    \n",
    "    # yolo folder firectory\n",
    "    output_dir = Path(output_dir)\n",
    "    (output_dir / 'train' / 'images').mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'train' / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'val' / 'images').mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'val' / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'test' / 'images').mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'test' / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # check images id\n",
    "    image_info = {img['id']: img for img in anns['images']}\n",
    "    \n",
    "    # get splits from original dataset\n",
    "    splits = anns['splits']['chessred2k']\n",
    "    train_ids = set(splits['train']['image_ids'])\n",
    "    val_ids = set(splits['val']['image_ids'])\n",
    "    test_ids = set(splits['test']['image_ids'])\n",
    "    \n",
    "    # process each image\n",
    "    for img_id, img_data in image_info.items():\n",
    "        # determine split\n",
    "        if img_id in train_ids:\n",
    "            split = 'train'\n",
    "        elif img_id in val_ids:\n",
    "            split = 'val'\n",
    "        elif img_id in test_ids:\n",
    "            split = 'test'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # origin and destination paths\n",
    "        src_img_path = os.path.join(root_dir, img_data['path'])\n",
    "        dst_img_dir = output_dir / split / 'images'\n",
    "        dst_label_dir = output_dir / split / 'labels'\n",
    "        \n",
    "        # copy image to the appropriate directory\n",
    "        shutil.copy(src_img_path, dst_img_dir)\n",
    "        \n",
    "        # generate label file name and path\n",
    "        img_width = img_data['width']\n",
    "        img_height = img_data['height']\n",
    "        txt_filename = Path(img_data['path']).stem + '.txt'\n",
    "        txt_path = dst_label_dir / txt_filename\n",
    "        \n",
    "        # annotations for the current image\n",
    "        img_annotations = [a for a in anns['annotations']['pieces'] if a['image_id'] == img_id]\n",
    "        \n",
    "        with open(txt_path, 'w') as f:\n",
    "            for ann in img_annotations:\n",
    "                if 'bbox' not in ann or 'category_id' not in ann:\n",
    "                    continue  # Invalid annotations\n",
    "                \n",
    "                yolo_class = ann['category_id']\n",
    "                \n",
    "                # normalizing bounding box coordinates\n",
    "                x_min, y_min, width, height = ann['bbox']\n",
    "                x_center = (x_min + width / 2) / img_width\n",
    "                y_center = (y_min + height / 2) / img_height\n",
    "                norm_width = width / img_width\n",
    "                norm_height = height / img_height\n",
    "                \n",
    "                f.write(f\"{yolo_class} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\")\n",
    "    \n",
    "    yaml_content = f\"\"\"path: {output_dir.absolute()}\n",
    "train: train/images\n",
    "val: val/images\n",
    "test: test/images\n",
    "\n",
    "names:\n",
    "0: white-pawn\n",
    "1: white-rook\n",
    "2: white-knight\n",
    "3: white-bishop\n",
    "4: white-queen\n",
    "5: white-king\n",
    "6: black-pawn\n",
    "7: black-rook\n",
    "8: black-knight\n",
    "9: black-bishop\n",
    "10: black-queen\n",
    "11: black-king\n",
    "12: empty\n",
    "\"\"\"\n",
    "\n",
    "    with open(output_dir / 'dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "convert_to_yolo_dataset('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = YOLO(\"yolo11n.pt\") \n",
    "\n",
    "results = model.train(data=\"yolo_dataset/dataset.yaml\", \n",
    "                      epochs=15, imgsz=640, device=\"cpu\", \n",
    "                      batch=5, project=\"chess_board_detection\", \n",
    "                      name=\"yolo11n_run\",\n",
    "                      patience=5,\n",
    "                      pretrained=True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_board_matrix(boxes, labels, offset=20, board_size=512, squares=8):\n",
    "    \"\"\"\n",
    "    Convert predicted bounding boxes and labels into an 8x8 board matrix,\n",
    "    adjusting for homography offset padding.\n",
    "    \n",
    "    Parameters:\n",
    "        boxes (Tensor or list of list): Nx4 format [[x1, y1, x2, y2], ...]\n",
    "        labels (Tensor or list): length-N label list\n",
    "        offset (int): pixel offset added during warp\n",
    "        board_size (int): total size of the board image (typically 512)\n",
    "        squares (int): number of squares per row/column (8 for standard chess)\n",
    "\n",
    "    Returns:\n",
    "        board_matrix: 8x8 numpy array with labels placed in appropriate squares\n",
    "    \"\"\"\n",
    "    board = np.zeros((squares, squares), dtype=int) - 1\n",
    "    square_size = (board_size - 2 * offset) / squares\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        cx = ((x1 + x2) / 2) - offset\n",
    "        cy = ((y1 + y2) / 2) - offset\n",
    "\n",
    "        col = int(cx // square_size)\n",
    "        row = int(cy // square_size)\n",
    "\n",
    "        # Clamp to stay within bounds\n",
    "        col = max(0, min(squares - 1, col))\n",
    "        row = max(0, min(squares - 1, row))\n",
    "\n",
    "        board[row, col] = int(label)\n",
    "\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"chess_board_detection/yolov9c_run/weights/best.pt\")\n",
    "\n",
    "image_path =os.path.join(test_dataset.root, test_dataset.file_names[91])\n",
    "# predict an image\n",
    "results = model.predict(\n",
    "    source=image_path\n",
    ")\n",
    "\n",
    "\n",
    "result = results[0]  # Get the first result\n",
    "boxes = result.boxes.xyxy  # Coor [x1, y1, x2, y2]\n",
    "labels = result.boxes.cls  # class id\n",
    "confs = result.boxes.conf   # confidence scores\n",
    "\n",
    "# Our board extraction\n",
    "image = read_image(image_path, False)\n",
    "filtered_images = apply_filters(image, False)\n",
    "chess_contour = get_contours(filtered_images, show=False,  kernel_size=(25,25) ,  kernel_usage=True, iterations=4)\n",
    "warped_image, M1 = rotate_and_crop(filtered_images, chess_contour[0][1], show=False)\n",
    "rotated_image, M2 = align_board(warped_image, radius=12, angle_step=90, show=False)\n",
    "M2_homography = np.vstack([M2, [0, 0, 1]])\n",
    "M_combined = M2_homography @ M1\n",
    "\n",
    "\n",
    "# === Warp image ===\n",
    "warp_img = cv2.warpPerspective(result.plot(), M_combined, (512, 512))\n",
    "\n",
    "warped_boxes = warp_boxes(boxes.cpu().numpy(), M_combined)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(warp_img)\n",
    "\n",
    "for box, label in zip(warped_boxes, labels):\n",
    "    x1, y1, x2, y2 = box\n",
    "    rect = Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                    linewidth=2, edgecolor='red', facecolor='none')\n",
    "    #ax.add_patch(rect)\n",
    "    #ax.text(x1, y1 - 5, str(label), color='yellow', fontsize=8, backgroundcolor='black')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = predictions_to_board_matrix(warped_boxes, labels, offset=45)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_chessboard(board_matrix, sprite_path, cell_size=80):\n",
    "    \"\"\"\n",
    "    Renders a chessboard from a matrix and individual piece images.\n",
    "\n",
    "    Args:\n",
    "        board_matrix (np.ndarray): 8x8 matrix with -1 for empty and 0-11 for pieces.\n",
    "        sprite_path (str): Directory containing PNG images for pieces.\n",
    "        cell_size (int): Size of each square on the board in pixels.\n",
    "    \"\"\"\n",
    "    # Create the output image\n",
    "    board_img = np.ones((8 * cell_size, 8 * cell_size, 4), dtype=np.uint8) * 255\n",
    "\n",
    "    # Load the piece images\n",
    "    piece_images = {}\n",
    "    for i in range(12):  # Pieces are indexed from 0 to 11\n",
    "        file_path = os.path.join(sprite_path, f\"{i}.png\")\n",
    "        if os.path.exists(file_path):\n",
    "            piece_img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)  # Load with alpha channel\n",
    "            piece_img = cv2.cvtColor(piece_img, cv2.COLOR_BGRA2RGBA)  # Convert to RGBA\n",
    "            piece_images[i] = piece_img\n",
    "\n",
    "    # Draw squares and pieces\n",
    "    for row in range(8):\n",
    "        for col in range(8):\n",
    "            x0 = col * cell_size\n",
    "            y0 = row * cell_size\n",
    "            color = (240, 217, 181) if (row + col) % 2 == 0 else (181, 136, 99)\n",
    "            board_img[y0:y0 + cell_size, x0:x0 + cell_size] = np.array(color + (255,), dtype=np.uint8)\n",
    "\n",
    "            piece_id = board_matrix[row][col]\n",
    "            if piece_id >= 0 and piece_id < 12:  # Ensure piece_id is valid\n",
    "                piece_img = piece_images.get(piece_id, None)\n",
    "                if piece_img is not None:\n",
    "                    piece_img_resized = cv2.resize(piece_img, (cell_size, cell_size))\n",
    "\n",
    "                    # Composite the piece image onto the board image\n",
    "                    for c in range(4):  # Iterate over RGBA channels\n",
    "                        board_img[y0:y0 + cell_size, x0:x0 + cell_size, c] = np.where(\n",
    "                            piece_img_resized[:, :, 3] > 0,  # Check if the alpha channel is not zero\n",
    "                            piece_img_resized[:, :, c],      # Use the piece image\n",
    "                            board_img[y0:y0 + cell_size, x0:x0 + cell_size, c]  # Keep the background\n",
    "                        )\n",
    "\n",
    "    # Convert to RGB for displaying with Matplotlib\n",
    "    board_img = cv2.cvtColor(board_img, cv2.COLOR_RGBA2RGB)\n",
    "    plt.imshow(board_img)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "    return board_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = render_chessboard(board, \"./chess_piece_sprites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov11n = \"chess_board_detection/yolo11n_run/weights/best.pt\"\n",
    "#best model yolov9c\n",
    "yolov9c = \"chess_board_detection/yolov9c_run/weights/best.pt\"\n",
    "model = YOLO(yolov9c)\n",
    "results = model.predict(\n",
    "    source=image_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(precisions, recalls):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recalls, precisions, marker='o')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iou_distribution(iou_scores):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(iou_scores, bins=30, range=(0, 1), color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('IoU')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('IoU Distribution')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_yolo_with_iou_and_collect(model, dataset, iou_threshold=0.5, conf_threshold=0.5):\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    iou_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    all_gt_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    total_mse = 0.0\n",
    "    matched_count = 0\n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        image_path = os.path.join(dataset.root, dataset.file_names[i])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image_id = dataset.image_ids[i]\n",
    "        gt_boxes = []\n",
    "        gt_labels = []\n",
    "\n",
    "        for ann in dataset.image_annotations[image_id]:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            gt_boxes.append([x, y, x + w, y + h])\n",
    "            gt_labels.append(ann['category_id'])\n",
    "\n",
    "        if len(gt_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        results = model.predict(image, conf=conf_threshold, verbose=False)\n",
    "        predictions = results[0]\n",
    "\n",
    "        if predictions.boxes is None or len(predictions.boxes.data) == 0:\n",
    "            total_fn += len(gt_boxes)\n",
    "            all_gt_labels.extend(gt_labels)\n",
    "            all_pred_labels.extend([-1] * len(gt_labels))  # -1 = no prediction\n",
    "            continue\n",
    "\n",
    "        pred_boxes = predictions.boxes.xyxy.cpu().numpy()\n",
    "        pred_labels = predictions.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "        matched_preds = set()\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        ious = ops.box_iou(torch.tensor(pred_boxes), torch.tensor(gt_boxes)).numpy()\n",
    "\n",
    "        for pred_idx in range(len(pred_boxes)):\n",
    "            best_iou_idx = np.argmax(ious[pred_idx])\n",
    "            best_iou = ious[pred_idx, best_iou_idx]\n",
    "\n",
    "\n",
    "        for gt_idx in range(len(gt_boxes)):\n",
    "            best_iou_idx = np.argmax(ious[:, gt_idx])\n",
    "            best_iou = ious[best_iou_idx, gt_idx]\n",
    "\n",
    "            if best_iou >= iou_threshold and best_iou_idx not in matched_preds:\n",
    "                tp += 1\n",
    "                matched_preds.add(best_iou_idx)\n",
    "                iou_scores.append(best_iou)\n",
    "\n",
    "                # Accuracy calculation\n",
    "                all_gt_labels.append(gt_labels[gt_idx])\n",
    "                all_pred_labels.append(pred_labels[best_iou_idx])\n",
    "\n",
    "                # MSE calculation\n",
    "                gt_box = np.array(gt_boxes[gt_idx])\n",
    "                mse = np.mean((pred_boxes[best_iou_idx] - gt_box) ** 2)\n",
    "                total_mse += mse\n",
    "                matched_count += 1\n",
    "            else:\n",
    "                # False negative (missed GT)\n",
    "                all_gt_labels.append(gt_labels[gt_idx])\n",
    "                all_pred_labels.append(-1)\n",
    "                iou_scores.append(0)\n",
    "\n",
    "        fn = len(gt_boxes) - tp\n",
    "\n",
    "        # false positives (predictions not matched)\n",
    "        for pred_idx in range(len(pred_boxes)):\n",
    "            if pred_idx not in matched_preds:\n",
    "                fp += 1\n",
    "                all_gt_labels.append(-1)\n",
    "                all_pred_labels.append(pred_labels[pred_idx])\n",
    "                iou_scores.append(0)\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "\n",
    "        precision = total_tp / (total_tp + total_fp + 1e-6)\n",
    "        recall = total_tp / (total_tp + total_fn + 1e-6)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    final_precision = total_tp / (total_tp + total_fp + 1e-6)\n",
    "    final_recall = total_tp / (total_tp + total_fn + 1e-6)\n",
    "    f1 = 2 * (final_precision * final_recall) / (final_precision + final_recall + 1e-6)\n",
    "\n",
    "    # Accuracy\n",
    "    # filtering eliminating -1 in all_gt_labels\n",
    "    filtered_gt = [gt for gt, pred in zip(all_gt_labels, all_pred_labels) if gt != -1]\n",
    "    filtered_preds = [pred for gt, pred in zip(all_gt_labels, all_pred_labels) if gt != -1]\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(filtered_gt, filtered_preds)\n",
    "\n",
    "    # MSE\n",
    "    avg_mse = total_mse / matched_count if matched_count > 0 else float('inf')\n",
    "\n",
    "    print(f\"Precision: {final_precision:.4f}\")\n",
    "    print(f\"Recall: {final_recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"MSE (boxes): {avg_mse:.4f}\")\n",
    "\n",
    "    return precisions, recalls, iou_scores, accuracy, avg_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, iou_scores, accuracy, avg_mse = evaluate_yolo_with_iou_and_collect(model, test_dataset)\n",
    "\n",
    "plot_precision_recall(precisions, recalls)\n",
    "plot_iou_distribution(iou_scores)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
