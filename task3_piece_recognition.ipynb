{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaU9_sOGzRD1"
   },
   "source": [
    "# Task 3: Piece Detection + Digital Twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvfTDUXuzRD9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np, os, torch, random, cv2, json\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.ops as ops\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDBhCwhszREA"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5IQVCmOzREB"
   },
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "data_aug = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_in = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juTsr3Lld8UT",
    "outputId": "4f307a21-f67a-4229-8c51-ea90ec364a25"
   },
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, root_dir, partition, transform=None):\n",
    "        self.root = root_dir\n",
    "        self.anns = json.load(open(os.path.join(root_dir, 'annotations.json')))\n",
    "\n",
    "        self.id_to_category = {c['id']: c['name'] for c in self.anns['categories']}\n",
    "        self.category_to_id = {c['name']: c['id'] for c in self.anns['categories']}\n",
    "        self.categories = [c['name'] for c in self.anns['categories']]\n",
    "\n",
    "        # --- Step 1: Pre-process ALL annotations and identify problematic images ---\n",
    "        self.all_image_ids_from_json = []\n",
    "        self.all_file_names_from_json = []\n",
    "        for img_info in self.anns['images']:\n",
    "            self.all_file_names_from_json.append(img_info['path'])\n",
    "            self.all_image_ids_from_json.append(img_info['id'])\n",
    "        self.all_image_ids_from_json = np.asarray(self.all_image_ids_from_json) # Convert to numpy for easy indexing/masking\n",
    "\n",
    "        problematic_image_ids = set() # Store image_ids that have any malformed piece annotation\n",
    "        # This will store ONLY VALID annotations, grouped by image_id\n",
    "        self.image_annotations = {}\n",
    "\n",
    "        for ann_idx, ann in enumerate(self.anns['annotations']['pieces']):\n",
    "            # Ensure required attributes for object detection are present\n",
    "            if 'bbox' not in ann or 'category_id' not in ann:\n",
    "                #print(f\"Warning: Annotation at index {ann_idx} for image_id {ann.get('image_id', 'N/A')} is malformed (missing 'bbox' or 'category_id'). This image will be excluded.\")\n",
    "                if 'image_id' in ann:\n",
    "                    problematic_image_ids.add(ann['image_id'])\n",
    "                continue # Skip this malformed annotation\n",
    "\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.image_annotations:\n",
    "                self.image_annotations[image_id] = []\n",
    "            self.image_annotations[image_id].append(ann)\n",
    "\n",
    "\n",
    "        # --- Step 2: Filter images based on split and problematic annotations ---\n",
    "        if partition == 'train':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['train']['image_ids']).astype(int)\n",
    "        elif partition == 'valid':\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['val']['image_ids']).astype(int)\n",
    "        else: # 'test'\n",
    "            raw_split_image_ids = np.asarray(self.anns['splits']['chessred2k']['test']['image_ids']).astype(int)\n",
    "\n",
    "        self.file_names = [] # Final list of file names for this split\n",
    "        self.image_ids = []  # Final list of original image IDs for this split\n",
    "\n",
    "        # Iterate through all images (which you already have indexed by self.all_image_ids_from_json)\n",
    "        for i, img_id in enumerate(self.all_image_ids_from_json):\n",
    "            # Check if image belongs to current split AND is not marked as problematic\n",
    "            if img_id in raw_split_image_ids and img_id not in problematic_image_ids:\n",
    "                # Also, ensure that the image actually has valid annotations after filtering.\n",
    "                # An image could be in the split, not problematic, but simply have no pieces.\n",
    "                # Or all its pieces were problematic, so it has no valid annotations left.\n",
    "                if img_id in self.image_annotations and len(self.image_annotations[img_id]) > 0:\n",
    "                    self.image_ids.append(img_id)\n",
    "                    self.file_names.append(self.all_file_names_from_json[i])\n",
    "                else:\n",
    "                    # Optional: Print why an image might be excluded if it's in the split but has no valid annotations\n",
    "                    print(f\"Info: Image ID {img_id} (file: {self.all_file_names_from_json[i]}) in '{partition}' split has no valid annotations after initial filtering. Excluding.\")\n",
    "\n",
    "        self.file_names = np.asarray(self.file_names)\n",
    "        self.image_ids = np.asarray(self.image_ids)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Number of {partition} images: {len(self.file_names)}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.file_names[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        original_image_id = self.image_ids[idx] # Get the true original ID for this image in the split\n",
    "        # Retrieve annotations using the pre-processed dictionary\n",
    "        annotations_for_image = self.image_annotations.get(original_image_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # Iterate through the (already filtered and valid) annotations for this image\n",
    "        for ann in annotations_for_image:\n",
    "            x_min, y_min, width, height = ann['bbox']\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) if len(labels) > 0 else torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        # Create the target dictionary required by Faster R-CNN\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([original_image_id])\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "train_dataset = ChessDataset('./chessred2k', 'train', data_aug)\n",
    "valid_dataset = ChessDataset('./chessred2k', 'valid', data_in)\n",
    "test_dataset = ChessDataset('./chessred2k', 'test', data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAYHXPnzd8UT",
    "outputId": "199a5237-a064-457b-da4f-a7184019595f"
   },
   "outputs": [],
   "source": [
    "# Device configuration (improved version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Hyperparameters (consider these adjustments)\n",
    "batch_size = 8  # Reduced from 16 for better memory management with Faster R-CNN\n",
    "num_workers = 0  # Optimal for most systems\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Enhanced collate function for object detection.\n",
    "    Handles:\n",
    "    - Empty targets\n",
    "    - Image tensor conversion\n",
    "    - Device movement\n",
    "    \"\"\"\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    processed_images = []\n",
    "    processed_targets = []\n",
    "    \n",
    "    for image, target in zip(images, targets):\n",
    "        # Convert image if not already a tensor\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Handle empty targets\n",
    "        if len(target['boxes']) == 0:\n",
    "            target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            # Verify bbox format (x1,y1,x2,y2)\n",
    "            assert target['boxes'].shape[1] == 4, \"Bounding boxes must have shape [N, 4]\"\n",
    "            \n",
    "        processed_images.append(image)\n",
    "        processed_targets.append(target)\n",
    "    \n",
    "    return processed_images, processed_targets\n",
    "\n",
    "# DataLoaders with improved settings\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Prevents partial batches\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True  # Keep all validation samples\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # Often better to use batch_size=1 for testing\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets) in enumerate(valid_dataloader):\n",
    "    print(f\"\\nBatch {i}:\")\n",
    "    for j, t in enumerate(targets):\n",
    "        print(f\"  Target {j}:\")\n",
    "        print(f\"    Boxes shape: {t['boxes'].shape}\")\n",
    "        print(f\"    Labels shape: {t['labels'].shape}\")\n",
    "        if t['boxes'].shape[0] != t['labels'].shape[0]:\n",
    "            print(\"    Inconsistency!\")\n",
    "    \n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the dataset is loaded correctly\n",
    "for i in range(len(valid_dataset)):\n",
    "    _, target = valid_dataset[i]\n",
    "    assert target['boxes'].dim() == 2 and target['boxes'].shape[1] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating dataset with pieces\n",
    "\n",
    "def plot_boxes(image, boxes, title=\"Detections\"):\n",
    "    plt.imshow(image)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso con tus datos:\n",
    "image, target = train_dataset[10]\n",
    "denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "             torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "denorm_img = np.clip(denorm_img, 0, 1)\n",
    "\n",
    "plot_boxes(denorm_img, target['boxes'], \"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aqWR_0VzRED"
   },
   "source": [
    "## Defining the model\n",
    "\n",
    "We will use a pre-trained Faster RCNN ResNet50 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfK3c9RSzRED",
    "outputId": "9cd92465-f107-4511-80a2-34f07fd16a33"
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "\n",
    "num_classes = 12  # 12 pieces + empty space\n",
    "    \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "step_size = 3 \n",
    "gamma = 0.1\n",
    "\n",
    "# Optimizador y scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=learning_rate, \n",
    "                     momentum=momentum, \n",
    "                     weight_decay=weight_decay)\n",
    "lr_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0).item()\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0).item()\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0).item()\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0).item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': total_loss/(progress_bar.n+1),\n",
    "            'Cls': loss_classifier/(progress_bar.n+1),\n",
    "            'Box': loss_box_reg/(progress_bar.n+1)\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0\n",
    "    \n",
    "    # Configuración especial para evitar errores de dimensiones\n",
    "    original_score_thresh = model.roi_heads.score_thresh\n",
    "    model.roi_heads.score_thresh = 0.0  # Desactiva filtrado inicial\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc='Validating')):\n",
    "        # 1. Conversión explícita y verificación\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{\n",
    "            'boxes': t['boxes'].float().to(device),\n",
    "            'labels': t['labels'].long().to(device)\n",
    "        } for t in targets]\n",
    "        \n",
    "        # 3. Forward pass con manejo de excepciones interno\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # 4. Cálculo de pérdida unificado\n",
    "        if isinstance(loss_dict, dict):\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        else:  # Caso raro de lista de diccionarios\n",
    "            losses = sum(sum(l.values()) for l in loss_dict)\n",
    "        \n",
    "        total_loss += losses\n",
    "        processed_batches += 1\n",
    "                \n",
    "    \n",
    "    # Restaura configuración original\n",
    "    model.roi_heads.score_thresh = original_score_thresh\n",
    "\n",
    "    return total_loss / processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "def train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs, early_stop_patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        train_loss = train_one_epoch(model, optimizer, train_dataloader, device, epoch)\n",
    "        \n",
    "        # Validación\n",
    "        val_loss = evaluate(model, valid_dataloader, device)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            patience_counter = 0\n",
    "            print(f\"¡Nuevo mejor modelo guardado! (val_loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping en epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Log de progreso\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(\"¡Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, optimizer, lr_scheduler, train_dataloader, valid_dataloader, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nms(predictions, iou_threshold=0.5):\n",
    "    filtered_preds = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        boxes = pred['boxes']\n",
    "        scores = pred['scores']\n",
    "        labels = pred['labels']\n",
    "\n",
    "        if boxes.numel() == 0:\n",
    "            filtered_preds.append(pred)\n",
    "            continue\n",
    "\n",
    "        # Apply NMS with no pre-filtering\n",
    "        keep_idxs = ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
    "\n",
    "        # Keep only those selected by NMS\n",
    "        filtered_preds.append({\n",
    "            'boxes': boxes[keep_idxs],\n",
    "            'scores': scores[keep_idxs],\n",
    "            'labels': labels[keep_idxs]\n",
    "        })\n",
    "\n",
    "    return filtered_preds\n",
    "\n",
    "def plot_predictions(model, dataset, num_samples=3):\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, target = dataset[idx]\n",
    "        img_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(img_tensor)\n",
    "        \n",
    "        # Procesamiento de la imagen para visualización\n",
    "        denorm_img = (image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)) + \\\n",
    "                     torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        denorm_img = denorm_img.permute(1, 2, 0).cpu().numpy()\n",
    "        denorm_img = np.clip(denorm_img, 0, 1)\n",
    "        \n",
    "        # Visualizar\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 7))\n",
    "        \n",
    "        # Ground Truth\n",
    "        ax[0].imshow(denorm_img)\n",
    "        for box in target['boxes']:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=1, edgecolor='g', facecolor='none'\n",
    "            )\n",
    "            ax[0].add_patch(rect)\n",
    "        ax[0].set_title('Ground Truth')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(denorm_img) \n",
    "        for box in prediction[0]['boxes']:\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=1, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            ax[1].add_patch(rect)\n",
    "        ax[1].set_title('Predictions')\n",
    "        ax[1].axis('off')\n",
    "        \n",
    "        # Predicciones\n",
    "        ax[2].imshow(denorm_img)\n",
    "\n",
    "        filtered_preds = custom_nms(prediction)   \n",
    "        for box in filtered_preds[0]['boxes']:\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=1, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            ax[2].add_patch(rect)\n",
    "        ax[2].set_title('Predictions')\n",
    "        ax[2].axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar disponibilidad de GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 12  # 12 pieces + empty space\n",
    "    \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "\n",
    "# Visualizar resultados\n",
    "plot_predictions(model, test_dataset, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, device, data_loader, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    total_mse = 0.0\n",
    "    matched_count = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc='Testing'):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{\n",
    "                'boxes': t['boxes'].float().to(device),\n",
    "                'labels': t['labels'].long().to(device)\n",
    "            } for t in targets]\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            for pred, gt in zip(predictions, targets):\n",
    "                pred_boxes = pred['boxes']\n",
    "                pred_labels = pred['labels']\n",
    "                pred_scores = pred['scores']\n",
    "\n",
    "                gt_boxes = gt['boxes']\n",
    "                gt_labels = gt['labels']\n",
    "\n",
    "                if pred_boxes.numel() == 0:\n",
    "                    # No predictions — all GTs are false negatives\n",
    "                    all_targets.extend(gt_labels.cpu().tolist())\n",
    "                    all_preds.extend([-1] * len(gt_labels))  # -1 = no prediction\n",
    "                    total_fn += len(gt_labels)\n",
    "                    continue\n",
    "\n",
    "                if gt_boxes.numel() == 0:\n",
    "                    # No GT — all predictions are false positives\n",
    "                    all_targets.extend([-1] * len(pred_labels))\n",
    "                    all_preds.extend(pred_labels.cpu().tolist())\n",
    "                    total_fp += len(pred_labels)\n",
    "                    continue\n",
    "\n",
    "                ious = ops.box_iou(gt_boxes, pred_boxes)  # [num_gt, num_pred]\n",
    "                gt_matched = set()\n",
    "                pred_matched = set()\n",
    "\n",
    "                for gt_idx in range(len(gt_boxes)):\n",
    "                    iou_row = ious[gt_idx]\n",
    "                    max_iou, pred_idx = torch.max(iou_row, dim=0)\n",
    "\n",
    "                    if max_iou >= iou_threshold and pred_idx.item() not in pred_matched:\n",
    "                        # Match found\n",
    "                        gt_label = gt_labels[gt_idx].item()\n",
    "                        pred_label = pred_labels[pred_idx].item()\n",
    "\n",
    "                        all_targets.append(gt_label)\n",
    "                        all_preds.append(pred_label)\n",
    "\n",
    "                        # Bounding box MSE\n",
    "                        mse = F.mse_loss(\n",
    "                            pred_boxes[pred_idx],\n",
    "                            gt_boxes[gt_idx],\n",
    "                            reduction='mean'\n",
    "                        ).item()\n",
    "                        total_mse += mse\n",
    "                        matched_count += 1\n",
    "\n",
    "                        gt_matched.add(gt_idx)\n",
    "                        pred_matched.add(pred_idx.item())\n",
    "                    else:\n",
    "                        # No good match found\n",
    "                        continue\n",
    "\n",
    "                # Count false negatives (GTs not matched)\n",
    "                for gt_idx in range(len(gt_boxes)):\n",
    "                    if gt_idx not in gt_matched:\n",
    "                        all_targets.append(gt_labels[gt_idx].item())\n",
    "                        all_preds.append(-1)\n",
    "                        total_fn += 1\n",
    "\n",
    "                # Count false positives (preds not matched)\n",
    "                for pred_idx in range(len(pred_boxes)):\n",
    "                    if pred_idx not in pred_matched:\n",
    "                        all_targets.append(-1)\n",
    "                        all_preds.append(pred_labels[pred_idx].item())\n",
    "                        total_fp += 1\n",
    "\n",
    "    # Filter out unmatched (-1) before computing classification metrics\n",
    "    y_true = torch.tensor([t for t in all_targets if t != -1])\n",
    "    y_pred = torch.tensor([p for t, p in zip(all_targets, all_preds) if t != -1])\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    avg_mse = total_mse / matched_count if matched_count > 0 else float('inf')\n",
    "\n",
    "    print(f\"\\nPrecision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"MSE (boxes): {avg_mse:.4f}\")\n",
    "\n",
    "    return precision, recall, f1, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, device, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
